{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import model_selection\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import operator\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "import time\n",
    "acc = make_scorer(accuracy_score)\n",
    "\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sampleData(DataFrame, ratio):\n",
    "    df_size = len(DataFrame.index)\n",
    "    sample_size = int(round(ratio * df_size))\n",
    "    if sample_size == df_size:\n",
    "        sample_size = sample_size - 1\n",
    "        indexes = random.sample(range(df_size), sample_size)\n",
    "        sample = DataFrame.iloc[indexes, : ]\n",
    "        return sample\n",
    "\n",
    "    elif sample_size == 0:\n",
    "        sample_size = 1\n",
    "        indexes = random.sample(range(df_size), sample_size)\n",
    "        sample = DataFrame.iloc[indexes, : ]\n",
    "        return sample\n",
    "\n",
    "    else:\n",
    "        indexes = random.sample(range(df_size), sample_size)\n",
    "        sample = DataFrame.iloc[indexes, : ]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "atlas = pd.read_csv('./Atlases/tissue_predictor_notfiltered_healthy_nofluid.csv', sep=\"/\")\n",
    "atlas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(atlas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas=atlas[atlas['tissue_name'] != 'Dental plaque']\n",
    "atlas=atlas[atlas['tissue_name'] != 'hMSC']\n",
    "atlas=atlas[atlas['tissue_name'] != 'Hela']\n",
    "atlas=atlas[atlas['tissue_name'] != 'Unknown']\n",
    "atlas = atlas.replace({'Cervix': 'Uterine cervix'})\n",
    "print(atlas.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the class balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas['tissue_name'].value_counts().plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas['tissue_name'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will balance the atlas a little bit by dropping the tissues that are too low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)  \n",
    "tissue_counts = atlas['tissue_name'].value_counts().to_frame()\n",
    "print(tissue_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop low tissues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_tissues = tissue_counts.index[tissue_counts['tissue_name']<3].tolist()\n",
    "atlas = atlas[~atlas['tissue_name'].isin(low_tissues)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = dict(Counter(atlas['tissue_name']))\n",
    "tf = sorted(tf.items(), key=operator.itemgetter(1), reverse=True)\n",
    "tf = dict(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using class weight in the predictors <br>\n",
    "To further balance the algorithm, the class will be assigned a specific weight based on the number of samples in this class. \n",
    "\n",
    "The weight of a class is determined by dividing negative samples/ positive samples. So if a class contains 15 samples in a dataset of 100 samples, the class weight will be 85/15=5,667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas=atlas.drop(columns=['cell_type', 'disease_status', 'fluid'])\n",
    "atlas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = atlas.iloc[:, :-1]\n",
    "y = atlas[['tissue_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = atlas.shape[0]\n",
    "weight_and_label = pd.DataFrame(columns=['label', 'weight'])\n",
    "i = 0\n",
    "for key, value in tf.items():\n",
    "    i += 1\n",
    "    w = (som - value)/value\n",
    "    weight_and_label.loc[i] = [key, w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame()\n",
    "test_df = pd.DataFrame()\n",
    "validation_df = pd.DataFrame()\n",
    "\n",
    "tissues = atlas['tissue_name'].unique()\n",
    "DataFrameDict = {elem : pd.DataFrame for elem in tissues}\n",
    "for key in DataFrameDict.keys():\n",
    "    DataFrameDict[key] = atlas[:][atlas['tissue_name'] == key]\n",
    "\n",
    "for key in DataFrameDict.keys():\n",
    "    train = sampleData(DataFrameDict[key], 0.80)\n",
    "    train_df = train_df.append(train)\n",
    "\n",
    "    test = DataFrameDict[key].drop(train.index)\n",
    "    test_df = test_df.append(test)\n",
    "\n",
    "y_train = train_df.pop('tissue_name').values\n",
    "X_train = train_df.values\n",
    "y_test = test_df.pop('tissue_name').values\n",
    "X_test = test_df.values\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=(atlas.columns)[:-1])\n",
    "X_test = pd.DataFrame(X_test, columns=(atlas.columns)[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_weight = pd.merge(pd.DataFrame(y_train, columns=['label']), weight_and_label, how='left', on='label')\n",
    "label_weight = pd.merge(pd.DataFrame(atlas['tissue_name'], columns=['label']), weight_and_label, how='left', on='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights are linked to the targets in the training data and in the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_weights = train_label_weight['weight'].to_numpy()\n",
    "train_weights = train_weights.flatten()\n",
    "train_weights\n",
    "\n",
    "weights = label_weight['weight'].to_numpy()\n",
    "weights = weights.flatten()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_train_label_weight = train_label_weight.drop_duplicates()\n",
    "dict_train_label_weights = dict(zip(train_label_weight.label, train_label_weight.weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing predictive algortihms performance on the training data <br>\n",
    "Using the cross val scores, several metrics of several algorithms are gathered to compare performance and choose 1 algorithm for further tuning.\n",
    "\n",
    "Each model had the necessary parameters to handle multilabel classification eg. the multisoftprob objective in XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that compares the CV perfromance of a set of predetrmined models \n",
    "#https://towardsdatascience.com/cross-validation-and-hyperparameter-tuning-how-to-optimise-your-machine-learning-model-13f005af9d7d\n",
    "def cv_comparison(models, names, X, y, cv):\n",
    "    # Initiate a DataFrame for the averages and a list for all measures\n",
    "    cv_scores = pd.DataFrame()\n",
    "    accs = []\n",
    "    f1s = []\n",
    "    precs = []\n",
    "    recs = []\n",
    "    f1s_w = []\n",
    "    precs_w = []\n",
    "    recs_w = []\n",
    "\n",
    "    # Loop through the models, run a CV, add the average scores to the DataFrame and the scores of \n",
    "    # all CVs to the list\n",
    "    for model, name in zip(models, names):\n",
    "        print(name)\n",
    "        start = time.time()\n",
    "        acc = np.round(cross_val_score(model, X, y, scoring='accuracy', cv=cv), 4)\n",
    "        accs.append(acc)\n",
    "        acc_avg = round(np.mean(acc[~np.isnan(acc)]), 4)\n",
    "        f1 = np.round(cross_val_score(model, X, y, scoring='f1_macro', cv=cv), 4)\n",
    "        f1s.append(f1)\n",
    "        f1_avg = round(np.mean(f1[~np.isnan(f1)]), 4)\n",
    "        prec = np.round(cross_val_score(model, X, y, scoring='precision_macro', cv=cv), 4)\n",
    "        precs.append(prec)        \n",
    "        prec_avg = round(np.mean(prec[~np.isnan(prec)]), 4)\n",
    "        rec = np.round(cross_val_score(model, X, y, scoring='recall_macro', cv=cv), 4)\n",
    "        recs.append(rec)        \n",
    "        rec_avg = round(np.mean(rec[~np.isnan(rec)]), 4)\n",
    "\n",
    "        f1_w = np.round(cross_val_score(model, X, y, scoring='f1_weighted', cv=cv), 4)\n",
    "        f1s_w.append(f1_w)\n",
    "        f1_w_avg = round(np.mean(f1_w[~np.isnan(f1_w)]), 4)\n",
    "        prec_w = np.round(cross_val_score(model, X, y, scoring='precision_weighted', cv=cv), 4)\n",
    "        precs_w.append(prec_w)        \n",
    "        prec_w_avg = round(np.mean(prec_w[~np.isnan(prec_w)]), 4)\n",
    "        rec_w = np.round(cross_val_score(model, X, y, scoring='recall_weighted', cv=cv), 4)\n",
    "        recs_w.append(rec_w)        \n",
    "        rec_w_avg = round(np.mean(rec_w[~np.isnan(rec_w)]), 4)\n",
    "        cv_scores[str(name)] = [acc_avg, f1_avg, prec_avg, rec_avg, f1_w_avg, prec_w_avg, rec_w_avg]\n",
    "        print(time.time() - start)\n",
    "    cv_scores.index = ['Accuracy', 'f1_macro', 'precision_macro', 'recall_macro', 'f1_weighted', 'precision_weighted', 'recall_weighted']\n",
    "    return cv_scores,accs, f1s, precs, recs, f1s_w, precs_w, recs_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes =len(np.unique(y_train))\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "log_unbal = LogisticRegression(random_state=42, multi_class='multinomial', n_jobs=-1)\n",
    "log=LogisticRegression(random_state=42, multi_class='multinomial', class_weight=dict_train_label_weights, n_jobs=-1)\n",
    "log_bal=LogisticRegression(random_state=42, multi_class='multinomial', class_weight='balanced', n_jobs=-1)\n",
    "\n",
    "rf_unbal = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "rf=RandomForestClassifier(random_state=42, class_weight=dict_train_label_weights, n_jobs=-1)\n",
    "rf_bal=RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "rf_sub=RandomForestClassifier(random_state=42, class_weight='balanced_subsample', n_jobs=-1)\n",
    "\n",
    "brf_unbal = BalancedRandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "brf = BalancedRandomForestClassifier(random_state=42, class_weight=dict_train_label_weights, n_jobs=-1)\n",
    "brf_bal = BalancedRandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "brf_sub = BalancedRandomForestClassifier(random_state=42, class_weight='balanced_subsample', n_jobs=-1)\n",
    "\n",
    "xgb_unbal = XGBClassifier(random_state=42,objective='multi:softprob', eval_metric='mlogloss', num_class=num_classes, n_jobs=-1)\n",
    "xgb=XGBClassifier(random_state=42, objective='multi:softprob', eval_metric='mlogloss', num_class=num_classes, weight=train_weights, n_jobs=-1)\n",
    "\n",
    "svm_unbal=SVC(random_state=42)\n",
    "svm=SVC(random_state=42, class_weight=dict_train_label_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[log_unbal, log, log_bal,rf, rf_bal, rf_sub, brf_unbal, brf, brf_bal, brf_sub,xgb_unbal, xgb,svm_unbal, svm]\n",
    "names = ['LogisticRegression unbalanced', 'LogisticRegression dict balanced', 'LogisticRegression balanced','RandomForest unbalanced', 'RandomForest dict balanced', 'RandomForest balanced', 'Random Forest balanced subsample', 'Balanced RandomForest',\n",
    "'Balanced RandomForest balanced','Balanced RandomForest balanced subsample','XGBClassifier unbalanced', 'XGBClassifier dict balanced','SVM unbalanced', 'SVM']\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=5, random_state=42)\n",
    "comp1, accs, f1s, precs, recs, f1s_w, precs_w, recs_w = cv_comparison(models, names, X_train, y_train, cv=cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "comp=pd.read_csv('comparison_full_data.csv', sep='\\t')\n",
    "comp = comp.set_index('Metrics')\n",
    "comp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = comp.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14,8))\n",
    "ax = sns.heatmap(ct, annot=True, cmap=\"magma\", fmt=\".3\")\n",
    "ax.xaxis.tick_top()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(columns=['model','Accuracy', 'f1_macro', 'precision_macro', 'recall_macro', 'f1_weighted', 'precision_weighted', 'recall_weighted'])\n",
    "result_df_cv = pd.DataFrame(columns=['model','fold', 'Accuracy', 'f1_macro', 'precision_macro', 'recall_macro', 'f1_weighted', 'precision_weighted', 'recall_weighted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest= RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "forest.fit(X_train, y_train)\n",
    "baseline_predictions =  forest.predict(X_test)\n",
    "fm = f1_score(y_test, baseline_predictions, average=\"macro\")\n",
    "fw = f1_score(y_test, baseline_predictions, average=\"weighted\")\n",
    "acc = accuracy_score(y_test, baseline_predictions)\n",
    "pw = precision_score(y_test, baseline_predictions, average='weighted')\n",
    "pm = precision_score(y_test, baseline_predictions, average='macro')\n",
    "rw = recall_score(y_test, baseline_predictions, average='weighted')\n",
    "rm = recall_score(y_test, baseline_predictions, average='macro')\n",
    "df_length = len(result_df)\n",
    "result_df.loc[df_length] = ['RandomForest_baseline', acc, fm, pm, rm, fw, pm, rw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_number = 0\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    fold_number += 1\n",
    "    train = atlas.iloc[train_index,:]\n",
    "    X_train_cv = train.iloc[:,:-1]\n",
    "    y_train_cv = train.iloc[:,-1]\n",
    "    test = atlas.iloc[test_index,:]\n",
    "    X_test_cv = test.iloc[:, :-1]\n",
    "    y_test_cv = test.iloc[:,-1]\n",
    "    forest = RandomForestClassifier(random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "    forest.fit(X_train_cv, y_train_cv)\n",
    "    importances = forest.feature_importances_\n",
    "    baseline_predictions =  forest.predict(X_test_cv)\n",
    "    fm = f1_score(y_test_cv, baseline_predictions, average=\"macro\")\n",
    "    fw = f1_score(y_test_cv, baseline_predictions, average=\"weighted\")\n",
    "    acc = accuracy_score(y_test_cv, baseline_predictions)\n",
    "    pw = precision_score(y_test_cv, baseline_predictions, average='weighted')\n",
    "    pm = precision_score(y_test_cv, baseline_predictions, average='macro')\n",
    "    rw = recall_score(y_test_cv, baseline_predictions, average='weighted')\n",
    "    rm = recall_score(y_test_cv, baseline_predictions, average='macro')\n",
    "    df_length = len(result_df_cv)\n",
    "    result_df_cv.loc[df_length] = ['RandomForest_baseline',fold_number, acc, fm, pm, rm, fw, pm, rw]\n",
    "print(result_df_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700, 750, 800, 850, 900, 950, 1000]\n",
    "max_features = ['log2', 'sqrt']\n",
    "max_depth = [1, 2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110]\n",
    "min_samples_split = [1, 2, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "min_samples_leaf = [1, 2, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "bootstrap = [True, False]\n",
    "param_dist = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rs = RandomizedSearchCV(forest,#sample weights are already in the forest algorithm so no need to add again\n",
    "param_dist,\n",
    "n_iter=100,\n",
    "cv=3,\n",
    "verbose=2,\n",
    "random_state=0)\n",
    "\n",
    "rs.fit(X_train, y_train)\n",
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_df = pd.DataFrame(rs.cv_results_).sort_values('rank_test_score').reset_index(drop=True)\n",
    "rs_df = rs_df.drop([\n",
    "            'mean_fit_time', \n",
    "            'std_fit_time', \n",
    "            'mean_score_time',\n",
    "            'std_score_time', \n",
    "            'params', \n",
    "            'split0_test_score', \n",
    "            'split1_test_score', \n",
    "            'split2_test_score', \n",
    "            'std_test_score'],\n",
    "            axis=1)\n",
    "rs_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [650,500,50,400,200]\n",
    "max_features = ['sqrt', 'log2']\n",
    "max_depth = [70,90,30,50]\n",
    "min_samples_split = [25,2,20,30]\n",
    "min_samples_leaf = [1,2,10]\n",
    "bootstrap = [True, False]\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "gs = GridSearchCV(forest,\n",
    "param_grid,\n",
    "cv=3,\n",
    "verbose=2)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_opt=RandomForestClassifier(n_estimators=650, max_depth=90, max_features='sqrt', bootstrap=False,\n",
    "    min_samples_leaf=1, min_samples_split=20, random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "forest_opt.fit(X_train, y_train)\n",
    "baseline_predictions =  forest_opt.predict(X_test)\n",
    "fm = f1_score(y_test, baseline_predictions, average=\"macro\")\n",
    "fw = f1_score(y_test, baseline_predictions, average=\"weighted\")\n",
    "acc = accuracy_score(y_test, baseline_predictions)\n",
    "pw = precision_score(y_test, baseline_predictions, average='weighted')\n",
    "pm = precision_score(y_test, baseline_predictions, average='macro')\n",
    "rw = recall_score(y_test, baseline_predictions, average='weighted')\n",
    "rm = recall_score(y_test, baseline_predictions, average='macro')\n",
    "df_length = len(result_df)\n",
    "result_df.loc[df_length] = ['RandomForest_optimised', acc, fm, pm, rm, fw, pm, rw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_number = 0\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    fold_number += 1\n",
    "    train = atlas.iloc[train_index,:]\n",
    "    X_train_cv = train.iloc[:,:-1]\n",
    "    y_train_cv = train.iloc[:,-1]\n",
    "    test = atlas.iloc[test_index,:]\n",
    "    X_test_cv = test.iloc[:, :-1]\n",
    "    y_test_cv = test.iloc[:,-1]\n",
    "    forest_optcv = RandomForestClassifier(n_estimators=650, max_depth=90, max_features='sqrt', bootstrap=False,\n",
    "    min_samples_leaf=1, min_samples_split=20, random_state=42, class_weight='balanced', n_jobs=-1)\n",
    "    forest_optcv.fit(X_train_cv, y_train_cv)\n",
    "    optimized_predictionscv =  forest_optcv.predict(X_test_cv)\n",
    "    fm = f1_score(y_test_cv, optimized_predictionscv, average=\"macro\")\n",
    "    fw = f1_score(y_test_cv, optimized_predictionscv, average=\"weighted\")\n",
    "    acc = accuracy_score(y_test_cv, optimized_predictionscv)\n",
    "    pw = precision_score(y_test_cv, optimized_predictionscv, average='weighted')\n",
    "    pm = precision_score(y_test_cv, optimized_predictionscv, average='macro')\n",
    "    rw = recall_score(y_test_cv, optimized_predictionscv, average='weighted')\n",
    "    rm = recall_score(y_test_cv, optimized_predictionscv, average='macro')\n",
    "    df_length = len(result_df_cv)\n",
    "    result_df_cv.loc[df_length] = ['RandomForest_optimised',fold_number, acc, fm, pm, rm, fw, pm, rw]\n",
    "print(result_df_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, y_test, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(np.unique(y_test)))\n",
    "    plt.xticks(tick_marks, np.unique(y_test), rotation=90)\n",
    "    plt.yticks(tick_marks, np.unique(y_test))\n",
    "    plt.tight_layout()\n",
    "    plt.grid(color='gainsboro')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, baseline_predictions)\n",
    "np.set_printoptions(precision=2)\n",
    "print('Confusion matrix, without normalization')\n",
    "print(cm)\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_confusion_matrix(cm, y_test)\n",
    "\n",
    "# Normalize the confusion matrix by row (i.e by the number of samples\n",
    "# in each class)\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "print('Normalized confusion matrix')\n",
    "print(cm_normalized)\n",
    "plt.figure(figsize=(20,12), dpi=1200)\n",
    "plot_confusion_matrix(cm_normalized, y_test, title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'tissue_predictor_RF_full_opt.pkl'\n",
    "pickle.dump(forest, open(filename, 'wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "59057a64ae204f2fdb51e9e1b433726989fd2ce2c53f4ab90fe5cecdfcc3bed6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('tissuespecific': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
